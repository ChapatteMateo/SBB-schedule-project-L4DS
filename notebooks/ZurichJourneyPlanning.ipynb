{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "root_data = \"../data/\"\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "ipython.run_cell_magic('configure','-f','{{ \"name\":\"final-{0}\" }}'.format(username))\n",
    "print(\"Runnuing as user: \", username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pyspark.sql.functions as SFunc\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that you can skip part I if you do not whish to rederive all the data from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Data Import & Wrangling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "hiveaddr = os.environ['HIVE_SERVER_2']\n",
    "print(\"Operating as: {0}\".format(username))\n",
    "print(\"Operating on hiveaddr: {0}\".format(hiveaddr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from pyhive import hive\n",
    "\n",
    "# create connection\n",
    "conn = hive.connect(host=hiveaddr, \n",
    "                    port=10000,\n",
    "                    username=username) \n",
    "# create cursor\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. First of all, if you have never done so, prepare the required orc tables in your personnal HDFS by running the notebook `PrepareTablesHDFS.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Processing the geostops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the geostops both to pre-process the SBB istdaten and the timetables. As the geostops is quite small, we will simply process it once in a pandas dataframe for SBB istdaten usage and once in a spark Dataframe to use on the cluster.  \n",
    "Note that alternatively we could store it on HDFS to prevent processing the dataset twice but given it's size and low complexity we decided not to. \n",
    "\n",
    "#######TODO: Ok or cleaner to change it again and create a HDFS table ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import all the geostops from the previously created orc table in a local pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "query = \"\"\"\n",
    "    select STATIONID as id, REMARK as name, LATITUDE as lat, LONGITUDE as lon\n",
    "    from {0}.sbb_geostops\n",
    "\"\"\".format(username)\n",
    "geostops_df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "geostops_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "data_types_dict = {'id': str, 'name': str, 'lat': float, 'lon': float}\n",
    "geostops_df = geostops_df.astype(data_types_dict)\n",
    "\n",
    "geostops_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we will filter this dataframe to only keep the stops that are within the studied 15km around ZurichHB area: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deal here with a short distance (15km) and our accuracy doesn't have to be exact to the centimeter, so we can treat the surface of the earth as flat.\n",
    "So to perform our check we can just make a conversion from degrees to kilometers at the latitude of the center point, then Pythagore's theorem to get the distance.\n",
    "\n",
    "We could also use methods offered by libraries such as geopy / geo-py but this adds unnecessary complexity and additional library to the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# Some constants to determine points within 15km from Zürich HB based on their (lat,lon) coordinates\n",
    "earth_radius = 6378.0\n",
    "zurich_avg_altitude = 0.430\n",
    "earth_circumference = 40075.0\n",
    "\n",
    "def distance(lat1, lon1, lat2, lon2, earth_circumference=earth_circumference):\n",
    "    \"\"\"\n",
    "    Computes the euclidean distance between two given points given their latitude and longitude coordinates\n",
    "    Code inspiration: https://stackoverflow.com/questions/24680247/check-if-a-latitude-and-longitude-is-within-a-circle-google-maps\n",
    "    \"\"\"\n",
    "    km_per_degree_lat = earth_circumference / 360.0\n",
    "    km_per_degree_lon = math.cos(math.pi * lat2 / 180.0) * km_per_degree_lat\n",
    "    dx = abs(lon2 - lon1) * km_per_degree_lon\n",
    "    dy = abs(lat2 - lat1) * km_per_degree_lat\n",
    "    return math.sqrt(dx*dx + dy*dy)\n",
    "\n",
    "def dist_from_center(lat_lon_row, central_lat=47.378177, central_lon=8.540192,earth_circumference=earth_circumference):\n",
    "    \"\"\"\n",
    "    Returns wether the distance of the given point (lat, long) from the central point (ZurichHB)\n",
    "    \"\"\"\n",
    "    return distance(lat_lon_row.lat, lat_lon_row.lon, central_lat, central_lon,earth_circumference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "max_dist=15.0\n",
    "geostops_df['center_dist'] = geostops_df.apply(dist_from_center, axis=1)\n",
    "zurich_geostops_df = geostops_df[geostops_df.center_dist <= max_dist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "zurich_geostops_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "zurich_geostops_df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by considering only stops within Zürich area, we keep 1947 stops over the total 39026."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "unique_stop_ids = len(set(zurich_geostops_df.id.tolist()))\n",
    "unique_stop_names = len(set(zurich_geostops_df.name.tolist()))\n",
    "\n",
    "print(\"Also remark that in those stops even so all %s stops have distinct Id, only %s have distinct names.\" %(unique_stop_ids, unique_stop_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# convert and save the dataframe to pickle\n",
    "pickle.dump(zurich_geostops_df, open(root_data+\"zurich_geostops_df.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Similar procedure to create Spark Dataframe for timetable processing usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earth_circumference = 40075.0\n",
    "\n",
    "@SFunc.udf\n",
    "def distance(lat1, lon1, lat2=47.378177, lon2=8.540192, earth_circumference=earth_circumference):\n",
    "    \"\"\"\n",
    "    Computes the euclidean distance between two given points given their latitude and longitude coordinates\n",
    "    Code inspiration: https://stackoverflow.com/questions/24680247/check-if-a-latitude-and-longitude-is-within-a-circle-google-maps\n",
    "    \"\"\"\n",
    "    km_per_degree_lat = earth_circumference / 360.0\n",
    "    km_per_degree_lon = math.cos(math.pi * lat2 / 180.0) * km_per_degree_lat\n",
    "    dx = abs(lon2 - lon1) * km_per_degree_lon\n",
    "    dy = abs(lat2 - lat1) * km_per_degree_lat\n",
    "    return math.sqrt(dx*dx + dy*dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist=15.0\n",
    "\n",
    "geostops = spark.read.orc(\"/data/sbb/orc/geostops\")\n",
    "geostops = geostops.withColumn('distance', distance(geostops['stop_lat'], geostops['stop_lon'])).filter(SFunc.col('distance') <= max_dist)\n",
    "geostops = geostops.drop('location_type', 'parent_station')\n",
    "geostops.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Processing the timetables data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a dataframe corresponding only to the schedules on May 13-17, 2019. As this is a typical week schedule, we will use it as our base timetable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetable = spark.read.csv(\"/data/sbb/csv/timetable/stop_times/2019/05/07/stop_times.csv\", header=True, encoding='utf8')\n",
    "timetable.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist of all we noted that most of those trips have duration under 1min (which makes sense for all bus stops close in location):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_count = timetable.count()\n",
    "long_trips_count = timetable[timetable.departure_time != timetable.arrival_time].count()\n",
    "print(\"Over the {0} trips, only {1} have duration higher than a minute.\".format(trips_count, long_trips_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's now only keep the trips that were made within our area of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all stop_id that are in Zurich\n",
    "zurich_stops = set([str(stop.stop_id) for stop in geostops.select('stop_id').collect()])\n",
    "\n",
    "\n",
    "#filter the timetable to only contains Stops that are in Zurich\n",
    "zurich_timetable=timetable.filter(F.col('stop_id').isin(zurich_stops))\n",
    "zurich_timetable.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition by trip_id order by arrival_time and get the next stop_id and next arrival_time\n",
    "\n",
    "rolling_pair_window=Window.partitionBy(\"trip_id\").orderBy(\"arrival_time\")\n",
    "\n",
    "next_arrival=F.lead(\"arrival_time\").over(rolling_pair_window).alias(\"arr_time\")\n",
    "next_stop_id=F.lead(\"stop_id\").over(rolling_pair_window).alias(\"arr_stop\")\n",
    "\n",
    "connections=zurich_timetable.select(\"trip_id\",\"departure_time\",\"stop_id\",next_arrival,next_stop_id)\\\n",
    "                            .na.drop(\"any\")\\\n",
    "                            .withColumnRenamed(\"stop_id\",\"dep_stop\")\\\n",
    "                            .withColumnRenamed(\"departure_time\",\"dep_time\")\n",
    "\n",
    "\n",
    "connections.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_foot_distance=0.5 # in km\n",
    "walking_speed_kmPerMin=0.05 # in km/min\n",
    "\n",
    "footpaths=geostops.alias('l').join(geostops.alias('r'))\\\n",
    "                .where('abs(r.distance- l.distance)<{0} and l.stop_id<>r.stop_id'.format(max_foot_distance))\\\n",
    "                .select(F.col('l.stop_id').alias('dep_stop'),\n",
    "                        F.col('r.stop_id').alias('arr_stop'),\n",
    "                        distance(F.col('l.stop_lat'),F.col('l.stop_lon'),F.col('r.stop_lat'),F.col('r.stop_lon')).alias('distance'))\\\n",
    "                .where('distance<{0}'.format(max_foot_distance))\\\n",
    "                .select('dep_stop',\n",
    "                        'arr_stop',\n",
    "                        (F.col('distance')/walking_speed_kmPerMin).alias('dur'))\n",
    "\n",
    "#duration is in minute\n",
    "footpaths.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################   \n",
    "This is to discuss but we could imagine further filtering the kind of trips we want to keep (ex: only keep trips within 2am and 11.30pm / ...)\n",
    "###############################  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create all the connections in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use Spark Windows ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Order them by departing time for CSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Processing the required istDaten SBB data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our model, we will first make an external table only containing all the journeys that:\n",
    "- Are between two stations within 15km of Zurich main train station ('Zürich HB (8503000)', lat=47.378177, lon=8.540192)\n",
    "- AN_PROGNOSE_STATUS and AB_PROGNOSE_STATUS equal to REAL or GESCHAETZT\n",
    "- Standard date of trip format\n",
    "- Non empty product id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "within_15_stop_stations = tuple(set(zurich_geostops_df.name.tolist()))\n",
    "\n",
    "query = \"\"\"\n",
    "    drop table if exists {0}.zurich_istdaten\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)\n",
    "\n",
    "query = \"\"\"\n",
    "    create external table {0}.zurich_istdaten\n",
    "    as\n",
    "    select FAHRT_BEZEICHNER as trip_id, lower(PRODUKT_ID) as ttype, LINIEN_ID as train_nb, FAELLT_AUS_TF as trip_failed, DURCHFAHRT_TF as no_stop,\n",
    "    HALTESTELLEN_NAME as stop_name, ZUSATZFAHRT_TF as unplanned_trip, LINIEN_TEXT as linien, VERKEHRSMITTEL_TEXT as verkehrsmittel,\n",
    "    unix_timestamp(ANKUNFTSZEIT, 'dd.MM.yyyy HH:mm') as expected_ar, unix_timestamp(AN_PROGNOSE,'dd.MM.yyyy hh:mm:ss') as actual_ar,\n",
    "    unix_timestamp(ABFAHRTSZEIT, 'dd.MM.yyyy HH:mm') as expected_dep, unix_timestamp(AB_PROGNOSE,'dd.MM.yyyy hh:mm:ss') as actual_dep\n",
    "    from {0}.sbb_orc\n",
    "    where BETRIEBSTAG like '__.__.____' and PRODUKT_ID is not NULL and PRODUKT_ID <> ''\n",
    "    and AN_PROGNOSE_STATUS in ('REAL', 'GESCHAETZT')\n",
    "    and AB_PROGNOSE_STATUS in ('REAL', 'GESCHAETZT')\n",
    "    and HALTESTELLEN_NAME in {1}\n",
    "\"\"\".format(username, within_15_stop_stations)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "query = \"\"\"\n",
    "    select *, floor((actual_ar-expected_ar)/(12))\n",
    "    from {0}.zurich_istdaten\n",
    "    where ttype = 'bus' and extract(hour from FROM_UNIXTIME(expected_ar)) = 12 and ((floor(expected_ar/86400) + 4) % 7+1) = 1\n",
    "    limit 5\n",
    "\"\"\".format(username)\n",
    "tr_sbb_df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "print(tr_sbb_df.columns)\n",
    "tr_sbb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can either do some more preprocessing or just work on the delays in the next part (II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Determine delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "!git lfs ls-files --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# Pull from git lfs our pickles\n",
    "!git lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "bucket_size = 12 # Set at 12, every delay (in seconds) will be grouped by bucket of size 12 seconds : [0,11], [12,25], .. \n",
    "query=\"\"\"\n",
    "    (select S.ttype as ttype,  S.day as day, S.hour as hour, S.delay as delay, count(*) as count \n",
    "    FROM (SELECT t.ttype, (floor(t.expected_ar/86400) + 4) % 7+1 as day, extract(hour from FROM_UNIXTIME(t.expected_ar)) as hour, floor((t.actual_ar-t.expected_ar)/{1})*{1}/60 as delay\n",
    "    FROM {0}.zurich_istdaten T) S\n",
    "    WHERE s.delay <= 8 and S.delay >= 0\n",
    "    GROUP BY S.ttype,  S.day, S.delay, S.hour\n",
    "    ORDER BY S.ttype,  S.day, S.delay, S.hour)\n",
    "\"\"\".format(username,bucket_size)\n",
    "dis = pd.read_sql(query, conn)\n",
    "\n",
    "ax = dis[(dis['ttype'] == 'bus') & (dis['day'] == 3) & (dis['hour'] == 8)].plot.bar(x='delay',y='count')\n",
    "ax.set_ylabel('Number of delays')\n",
    "ax.set_xlabel('Delay in minutes')\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "cdf = {}\n",
    "\n",
    "for ttype in ['bus','zug']:\n",
    "    cdf_day = {}\n",
    "    for day in [1,2,3,4,5,6,7]:\n",
    "        cdf_hour = {}\n",
    "        for hour in range(6,22):\n",
    "            hist = dis[(dis['ttype'] == ttype) & (dis['day'] == day) & (dis['hour'] == hour)]['count'].to_numpy()\n",
    "            while len(hist) < 41 and len(hist) != 0 :\n",
    "                hist = np.append(hist,0.0)\n",
    "\n",
    "            cdf_hour[hour] = np.cumsum(hist/hist.sum())\n",
    "        cdf_day[day] = cdf_hour\n",
    "    cdf[ttype]=cdf_day\n",
    "\n",
    "    \n",
    "# Cumulative Distribution Function of the delay (not arrival time, the *delay*) of a BUS on WEDNESDAY at 10:00\n",
    "plt.ylabel('P(delay < x)')\n",
    "plt.xlabel(\"Delay in Minutes\")\n",
    "plt.plot(np.linspace(0,8,41),cdf['bus'][3][10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Find the best journeys: the Connection Scan Algorithm (CSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "range(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
